["Published as a conference paper at ICLR 2024\nAMORTIZING INTRACTABLE INFERENCE\nIN LARGE LANGUAGE MODELS\nEdward J. Hu*, Moksh Jain*, Eric Elmoznino\nMila – Quebec AI Institute, Universit´ e de Montr´ eal\n{edward.hu,moksh.jain,eric.elmoznino,...\nYounesse Kaddar∞\nUniversity of Oxford\nyounesse.kaddar@chch.ox.ac.uk\nGuillaume Lajoie†, Yoshua Bengio⋄, Nikolay Malkin\nMila – Quebec AI Institute, Universit´ e de Montr´ eal\n...,guillaume.lajoie,yoshua.bengio,nikolay.malkin}@mila.quebec\narXiv:2310.04363v2 [cs.LG] 13 Mar 2024\nABSTRACT\nAutoregressive large language models (LLMs) compress knowledge from their\ntraining data through next-token conditional distributions. This limits tractable\nquerying of this knowledge to start-to-end autoregressive sampling. However, many\ntasks of interest—including sequence continuation, infilling, and other forms of\nconstrained generation—involve sampling from intractable posterior distributions.\nWe address this limitation by using amortized Bayesian inference to sample from\nthese intractable posteriors. Such amortization is algorithmically achieved by fine-\ntuning LLMs via diversity-seeking reinforcement learning algorithms: generative\nflow networks (GFlowNets). We empirically demonstrate that this distribution-\nmatching paradigm of LLM fine-tuning can serve as an effective alternative to\nmaximum-likelihood training and reward-maximizing policy optimization. As an\nimportant application, we interpret chain-of-thought reasoning as a latent variable\nmodeling problem and demonstrate that our approach enables data-efficient adapta-\ntion of LLMs to tasks that require multi-step rationalization and tool use.\nCode: https://github.com/GFNOrg/gfn-lm-tuning.\n1 INTRODUCTION\nAutoregressive large language models (LLMs) trained on general-domain data are vast stores of\nworld knowledge (Petroni et al., 2019). They are typically optimized by predicting a token given\nits preceding context; therefore, tractable inference over this knowledge is limited to sampling\nconditioned on a prefix. Many useful tasks, such as infilling (Zhu et al., 2019; Liu et al., 2019),\ngenerating text conditioned on length or lexical constraints (Hokamp & Liu, 2017; Hu et al., 2019),\nand finding the most likely sequence continuation, involve intractable inference in LLMs.\nSuch tasks are related to the problem of reasoning, which has been framed as one of probabilistic\ninference (Gershman & Goodman, 2014). Correspondingly, the linguistic expression of reasoning\ncan be seen as inference over language. For example, we can interpret chain-of-thought reasoning\n(Wei et al., 2022; Kojima et al., 2022), a paradigm of reasoning in language models, as a problem\nof intractable posterior inference. Given a question-answer pair (𝑋,𝑌), we are interested in finding\nlatent chains of thought – token sequences 𝑍that contribute the most to the conditional likelihood\n𝑝(𝑌 |𝑋)= ∑︁\n𝑝LM (𝑍𝑌 |𝑋)= ∑︁\n𝑝LM (𝑌 |𝑋𝑍)𝑝LM (𝑍 |𝑋), (1)\n𝑍\n𝑍\nwhere 𝑝LM denotes the likelihood assigned to a sequence by a language model and apposition of\nvariables (e.g., 𝑋𝑍𝑌) denotes the concatenation of the token sequences.\nWhile past work has relied on prompting and in-context learning to produce 𝑍’s that lead to the\ncorrect 𝑌, treating 𝑍 as a hidden variable in a latent variable model (LVM) renders chain-of-thought\nreasoning a Bayesian inference problem (Fig. 1). For this LVM, the distribution we must sample\nfrom is the posterior 𝑝LM (𝑍 |𝑋,𝑌)= 𝑝LM (𝑋𝑍𝑌)\n𝑍′ 𝑝LM (𝑋𝑍′𝑌). Such sampling is intractable: while it is easy\nto evaluate 𝑝LM (𝑋𝑍𝑌), the conditional distributions needed to sample 𝑍 from 𝑝LM (𝑍 |𝑋,𝑌)one\ntoken at a time are not easy to compute.\n∗Equal contribution.∞Work done during internship at Mila. †CIFAR AI Chair. ⋄CIFAR Senior Fellow.\n1\nPublished as a conference paper at ICLR 2024\n𝑍\nShe caught and ate a mouse. / She meowed until she was fed. / . . .\n𝑋 𝑌\n𝑋\nThe cat was hungry. Now the cat is sleepy, not hungry.\n𝑌 GFlowNet\n(as fine-tuned LM)\nThe review expresses a personal opinion.\neval(3 + 4)= 7, eval(7− 8)=−1\n𝑞GFN (𝑍 |𝑋 [, 𝑌 ])\n“A deeply moving storyline.” Label: Subjective\n3 + 4− 8 Answer:−1\n𝑍\nreward\n𝑅(𝑍, 𝑋, 𝑌 )\nFigure 1: Left: Three problems of reasoning in language – sentence infilling, chain-of-thought reasoning, and\nproblem-solving with external tool use – can all be seen as instances of the latent variable model at the top left,\nwhere an input (𝑋) generates the output (𝑌) via a latent variable (𝑍). Right: We fine-tune an LLM to sample\nfrom the Bayesian posterior over 𝑍, conditioning on 𝑋 and optionally on 𝑌. If conditioned on 𝑌, the trained\npolicy can be used to sample diverse latent sequences (e.g., for infilling, §4.2). If not conditioned on 𝑌, the policy\ncan sample 𝑍, and thus predict 𝑌, for inputs 𝑋not seen during training (e.g., for classification and multi-step\nreasoning, §4.3, 4.4). As shown in §4.4, modeling the full diversity of the posterior aids generalization.\nA standard method to sample approximately from intractable posterior distributions is Markov\nchain Monte Carlo (MCMC), but it is difficult to craft good proposal distributions for multi-modal\ndistributions over language data (Miao et al., 2019; Zhang et al., 2020a; Lew et al., 2023), and\ninference on a new input may be prohibitively slow. Alternatively, one can turn to reinforcement\nlearning (RL) approaches such as proximal policy optimization (PPO; Schulman et al., 2017), where\nthe language model is treated as a policy to be fine-tuned. However, these do not aim to model the\nfull diversity of the distribution; instead, learned policies settle around a small number of modes. In\nboth cases, issues with this mode collapse are exacerbated when the target distribution is misspecified,\nleading to the undesirable behavior of overoptimized samplers (Gao et al., 2023).\nAmortized probabilistic inference – that is, training a model to approximate a distribution of interest –\nprovides a principled, efficient, and potentially scalable way to draw samples from the distribution\n(Beal, 2003). One way to implement amortized inference for high-dimensional discrete data such as\ntext is using generative flow networks (GFlowNets; Bengio et al., 2021), which are diversity-seeking\nreinforcement learning algorithms that train policies to sample objects (such as a token sequence 𝑍)\nwith probability proportional to a given reward function, such as the joint 𝑝LM (𝑋𝑍𝑌).\nIn this work, we present a method that initializes the GFlowNet policy with a pretrained LLM\nand continues to train it with a reward objective that can be evaluated with the same LLM. The\nresult is a different type of fine-tuning (FT) procedure for text generation that has a number of\nadvantages, including improved sample diversity, data efficiency, and out-of-distribution generaliza-\ntion. GFlowNet fine-tuning makes the language model sample from the target distribution, enabling\namortized inference in a number of applications (Fig. 1).\nLeveraging this approach, we empirically demonstrate the possibilities and benefits of learning to\nsample from intractable distributions over text continuations, latent reasoning chains, and tool use\nsequences using GFlowNet fine-tuning. Notably, the diversity of samples from the models trained with\nGFlowNet fine-tuning is beneficial in Bayesian model averaging settings, such as when aggregating\nanswers to questions obtained via multiple reasoning chains. For example, using a pretrained language\nmodel with 6B parameters, our method shows an absolute improvement of 10.9% over supervised\nfine-tuning on subjectivity classification with only 10 labeled examples (§4.3) and outperforms\nsupervised fine-tuning and PPO by 63% on integer arithmetic with 50 demonstrations, with notable\nimprovements in out-of-distribution generalization (§4.4). Moreover, the benefits of amortized\ninference allow us to efficiently sample from the fine-tuned model at scale. Our contributions include:\n(1) A general algorithm for amortized sampling from intractable LLM posteriors.\n(2) A probabilistic approach to fine-tuning LLMs to perform chain-of-thought reasoning.\n(3) Empirical results on sequence continuation, natural language reasoning, integer arithmetic with\ntool use, and story infilling.\n2 MOTIVATING EXAMPLE: GENERATING RANDOM NUMBERS WITH LLMS\nWe consider a simple task that highlights the limitations of reward-maximizing reinforcement learning\n(RL) methods in fine-tuning LLMs. For readers unfamiliar with RL, we refer to Sutton & Barto\n(2018) and include a glossary in §A.1 to define key terms used throughout this paper. The task\ninvolves generating random numbers from a given distribution when prompted ‘The following is a\nrandom integer drawn uniformly between 0 and 100: ’. This task is a minimal instantiation of the\nproblem we study in the paper: sample from a target distribution given an unnormalized density.\n2\nProbability mass\nPublished as a conference paper at ICLR 2024\n100\n10 1\n10 2\n10 3\n10 4\n10 5\n0 20 40 Generated 60 number\n80 100\n0 20 40 Generated 60 number\n80 100\n0 20 40 Generated 60 number\n80 100\n(c) (a) Base model\n50.5% of samples\nare valid numbers.\n(b) PPO fine-tuning\n95.8% of samples\nare valid numbers.\nGFlowNet fine-tuning\n100% of samples\nare valid numbers.\nFigure 2: Empirical distributions of 512,000 integers from 1 to 100 generated by GPT-J (reward-maximizing; b) and GFlowNet fine-tuning (distribution-matching; c). Note the fine-tuned with PPO\nlogarithmic 𝑦-scale.\nAlthough the target distribution is tractable, making the task seemingly straightforward, it serves as a\nuseful illustration of the behaviors of different fine-tuning methods.\nRenda et al. (2023) found that pretrained LLMs perform quite poorly on this task: the distribution of\nnumbers generated with the above prompt will be far from uniform (Fig. 2a shows an example using\nan instruction fine-tuned GPT-J 6B (Wang & Komatsuzaki, 2021)1). There may be many reasons for\nthis, among them the effects of instruction fine-tuning and the LLM’s possible bias towards numbers\nthat are more frequent in the training data (e.g., numbers starting with ‘1’ are more frequent due to\nthe properties of many natural data-generating processes (Benford, 1938)).\nWhile reward-maximizing RL can teach the model to generate valid numbers (by penalizing outputs\nthat are not numbers from 1 to 100), it would not resolve the distribution skew introduced during\npretraining. Indeed, rewarding all valid integers equally leads to an expected gradient of zero for\npolicy gradient methods. Fig. 2b shows that while most samples are valid numbers after PPO training,\nthe distribution remains highly skewed.\nInstead, we can take a principled approach by training the LLM to match the target distribution with\na GFlowNet learning objective. Such an objective directly optimizes the likelihood of the model gen-\nerating a number to be proportional to the reward for that number, which is the number’s (potentially\nunnormalized) probability under the target distribution. When the policy is initialized as the pretrained\nLLM, the resulting distribution after GFlowNet fine-tuning is shown in Fig. 2c. Quantitatively, the\nKL divergence from the sampling distribution to the target (uniform) distribution decreases from 3.37\nfor the original LLM (on the support [0,100]) to 9.75·10−5 for the GFlowNet-fine-tuned model.\nThis example illustrates a general point: GFlowNet objectives provide a principled and flexible\napproach to fine-tuning LLMs to match a target distribution where reward-maximizing RL fails to.\nOn this simple task, this distribution matching could also be achieved through supervised fine-tuning;\nhowever, this would require access to samples from the target distribution, which are unavailable in\ngeneral (though not in this simple example). In the following sections, we further illustrate this point\nin non-trivial problems involving intractable inference, reasoning with latent variables, and tool use.\n3 FINE-TUNING LLMS TO SAMPLE FROM INTRACTABLE DISTRIBUTIONS\nWe first describe how intractable inference emerges from interesting applications of LLMs, one\nof which is chain-of-thought reasoning seen through the lens of latent variable models, where the\nposterior distribution over the latent variable is intractable. We then discuss how GFlowNet objectives\ncan be used to train amortized samplers to perform such intractable inference.\n3.1 PROBLEM: INTRACTABLE INFERENCE IN LARGE LANGUAGE MODELS\nAutoregressive language models decompose the distribution over sequences of tokens as a product of\nordered conditionals: 𝑝(𝑤1:𝑁)= 𝑝(𝑤1)𝑝(𝑤2 |𝑤1)···𝑝(𝑤𝑁 |𝑤1:𝑁−1). While this decomposition\nmakes left-to-right sampling from the distribution tractable, sampling from other conditional distribu-\nmodel available at .co/nlpcloud/instruct-gpt-j-fp16.\n1We use the Instruct-GPT-J 3\nPublished as a conference paper at ICLR 2024\nTable 1: Objects in language posterior inference. Given a pretrained ‘teacher’ LM 𝑝LM, we train a GFlowNet\n𝑞GFN to sample the posterior 𝑝(𝑍 |𝑋,𝑌). Amortization and generalization are achieved by making 𝑋, and\noptionally 𝑌, an input to 𝑞GFN.\nObject Meaning Example 1 (infilling) Example 2 (subjectivity classification)\n𝑋 cause / condition / question The cat was hungry. A deeply moving storyline.\n𝑍 mechanism / reasoning chain She ate a mouse. This review expresses personal feelings.\n𝑌 effect / answer Now the cat is sleepy, not hungry. Answer: Subjective\n𝑝(𝑍 |𝑋) conditional prior 𝑝LM (𝑍 |𝑋)\n𝑝(𝑌 |𝑋,𝑍) likelihood of effect given\n𝑝LM (𝑌 |𝑋𝑍)\ncause and mechanism\n𝑝(𝑍,𝑌 |𝑋) conditional joint, reward for 𝑍 𝑝LM (𝑍𝑌 |𝑋)\n𝑝(𝑍 |𝑋,𝑌) 𝑞(𝑌 |𝑋) posterior (intractable!) posterior predictive / Bayesian model average approximated and amortized by GFlowNet 𝑞GFN (𝑍 |𝑋[,𝑌])\napproximated as 𝑍 𝑞GFN (𝑍 |𝑋)𝑝LM (𝑌 |𝑋𝑍),\nsampled as 𝑍∼𝑞GFN (𝑍 |𝑋), 𝑌∼𝑝LM (𝑌 |𝑋𝑍)\ntions is intractable. Various problems of language modeling can be viewed as sampling from such\nintractable conditionals in the distribution over sequences of an LLM; we give two such examples\nand related terminologies in Table 1. Some tasks we study in §4 are instances of these examples.\nTempered and contrastive sampling. In many applications (e.g., translation, summarization,\ndialogue systems), one wishes to sample from a low-temperature distribution over sequences 𝑍\nconditioned on a prefix 𝑋, i.e., 𝑞(𝑍 | 𝑋)∝𝑝LM (𝑋𝑍)1/𝑇 for some temperature 𝑇 < 1, as high-\nlikelihood samples are more likely to be fluent or accurate continuations of 𝑋 (Tillmann & Ney,\n2003). The limit of 𝑇 →0 gives a distribution that is peaky on the most likely continuation. However,\nsampling from 𝑞, or finding its mode, is intractable, and it is common to resort to approximations,\nsuch as tempering the tokenwise conditional distributions or using beam search to search for a mode.\nA related problem is sampling a continuation with a correction for its unconditional likelihood,\ne.g., 𝑞(𝑍 |𝑋)∝𝑝LM (𝑋𝑍)𝛼𝑝LM (𝑍)𝛽 with 𝛽 < 0 and 𝛼 > 0, where applications again resort to\napproximating the next-token conditionals of 𝑞by tempering (Malkin et al., 2022b; Li et al., 2023).\nInfilling and reverse generation. Infilling is the task of sampling a sequence of tokens conditioned\non both its prior and subsequent context, which can be understood as sampling from the distribution\n𝑞(𝑍 |𝑋,𝑌)∝𝑝LM (𝑋𝑍𝑌), where 𝑋and 𝑌 are fixed. Reverse generation is a special case, where 𝑋\nis an empty sequence. Besides being a meaningful task in its own right (Liu et al., 2019; Zhu et al.,\n2019; Donahue et al., 2020; Susanto et al., 2020; Lu et al., 2022a), infilling and reverse generation\nare key components of newly emerging methods of LLM prompting, such as when LLMs are tasked\nwith optimizing their own instruction sequences or reasoning steps (Zhou et al., 2023; Sordoni et al.,\n2023; Xu et al., 2023). Current applications achieve this by resorting to hand-engineered instructions\nand inverted prompts.\nConstrained generation. Sampling of text with constraints and penalties – for example, those on\nthe presence or the absence of certain words or on the score of an auxiliary classifier evaluated on\nthe text – can be understood as sampling from a distribution 𝑞(𝑍)∝𝑝LM (𝑍)𝑐(𝑍), where 𝑐is an\nexternally specified constraint. Current approaches to the problem use tokenwise approximations\n(Liu et al., 2021), various problem-specific beam search and local search techniques (e.g., Schmaltz\net al., 2016; Hokamp & Liu, 2017; Hu et al., 2019; Sha, 2020; Lu et al., 2022b) or classifier-guided\nconditional generation approaches (e.g., Yang & Klein, 2021; Meng et al., 2022).\n3.2 REASONING THROUGH LATENT VARIABLES\nChain-of-thought reasoning (Wei et al., 2022; Kojima et al., 2022) helps LLMs solve complex\nproblems by producing a reasoning chain before giving the final answer. LLMs pretrained on general\ndomain data can learn to produce useful chains of thoughts given demonstrations, which are usually\nhandcrafted or generated by prompting the LM. Interestingly, although the capacity for chain-of-\nthought reasoning only emerges in large language models, knowledge can also be extracted from\nsmaller language models when they are carefully fine-tuned (Schick & Sch¨ utze, 2021).\nMotivated by this, we connect chain-of-thought reasoning to the general problem of inference in latent\nvariable models illustrated in Fig. 1. Here, reasoning can be seen as posterior inference: sampling\nfrom the posterior distribution over a string of tokens 𝑍 conditioned on a prefix 𝑋 and a suffix 𝑌,\ngiven an autoregressive language model 𝑝LM. The posterior is defined as\n𝑝LM (𝑍 |𝑋,𝑌)=\n𝑝LM (𝑋𝑍𝑌)\n∝𝑝LM (𝑋𝑍𝑌). (2)\n𝑍′ 𝑝LM (𝑋𝑍′𝑌)\nOur goal is to train models to sample 𝑍 from this posterior distribution. Intuitively, this allows us to\nsample likely reasoning chains that lead to the desired outcome 𝑌. Although we take 𝑍 to be a string\n4\nPublished as a conference paper at ICLR 2024\nof tokens, the same formalism and the GFlowNet objectives apply to other structured latent objects,\nsuch as trees or sets of natural language statements, as long as one has access to a likelihood model\n𝑝(𝑌 |𝑋𝑍). While not investigated in this work, these generalizations could be important for formal\nreasoning and multi-step chains of inference. See, e.g., Yao et al. (2023); Hao et al. (2023); Besta\net al. (2024) for approaches to reasoning in language using tree- or list-structured state spaces.\nA latent variable model of this form is useful when the marginal distribution 𝑝LM (𝑌 |𝑋)is harder\nto model than 𝑝LM (𝑍 |𝑋)and 𝑝LM (𝑌 |𝑋𝑍), i.e., a difficult inference is broken down into a chain\nof easier ones. By training a model to match the Bayesian posterior 𝑝LM (𝑍 |𝑋,𝑌), we can learn to\nsample latent reasoning chains that increase the likelihood of producing 𝑌 from 𝑋via the sampled 𝑍.\nHowever, we can also fine-tune the language model 𝑝LM (𝑍 |𝑋𝑌)itself to maximize the likelihood\nof data pairs (𝑋,𝑌)under the LVM. While it is generally intractable to directly maximize the\ndata likelihood 𝑝LM (𝑋,𝑌)= 𝑍 𝑝LM (𝑋𝑍𝑌)because of the summation over 𝑍, the (variational)\nexpectation-maximization (EM) algorithm (Dempster et al., 1977; Beal, 2003; Koller & Friedman,\n2009) can be used for this purpose. In the expectation step (E-step), we draw samples from the\nposterior over the latent variable 𝑝LM (𝑍 |𝑋,𝑌), which could come from an amortized sampler of 𝑍.\nIn the maximization step (M-step), we maximize the log-likelihood of the joint probability of the\nsampled latent variables E𝑍∼𝑝LM (𝑍|𝑋,𝑌)log 𝑝LM (𝑋𝑍𝑌)with respect to the parameters of the language\nmodel 𝑝LM. This combination of amortized inference (learning to sample the chain of thought) and\nsupervised fine-tuning (optimizing the language model with the ‘supervision’ involving 𝑍sampled\nfrom the amortized posterior) will be illustrated in one of our experiments (§4.3, Table 3).\n3.3 AMORTIZED INFERENCE WITH GFLOWNET OBJECTIVES\nFor inference in the latent variable model, we leverage the probabilistic framework of generative\nflow networks (GFlowNets; Bengio et al., 2021; 2023). Using notation from Malkin et al. (2022a),\nwe briefly introduce relevant GFlowNet concepts pertaining to autoregressive sequence generation.\nHere, GFlowNets learn policies to sample sequences 𝑍= 𝑧1 𝑧2 ...𝑧𝑛⊤∈Z(where ⊤denotes a stop\nsymbol) from a distribution over the space of sequences Z, given an unnormalized density (reward)\n𝑅: Z→R>0. The generative process is the same as in autoregressive language models: generation\nbegins with an empty string, and at the 𝑖-th step a token 𝑧𝑖 is sampled from a policy 𝑞GFN (𝑧𝑖 |𝑧1:𝑖−1),\nwhich is then appended to the sequence. This process continues until a stop symbol ⊤is generated.\nThe marginal likelihood 𝑞⊤\nGFN (𝑍)of sampling a terminal state 𝑍= 𝑧1:𝑛⊤is given by𝑛\n𝑖=1 𝑞GFN (𝑧𝑖 |\n𝑧1:𝑖−1)𝑞GFN (⊤|𝑧), where 𝑧1:0 is understood to be the empty string. The goal of GFlowNet training is\nto fit a parametric policy 𝑞GFN (·|·; 𝜃)such that 𝑞⊤\nGFN (𝑍)∝𝑅(𝑍), i.e., the likelihood of generating a\ncomplete sequence is proportional to its reward.\nLearning objective. We use a modified version of the subtrajectory balance (SubTB; Madan et al.,\n2023) objective to account for trajectories being terminable at all states (Deleu et al., 2022). The\nobjective for a sequence 𝑍= 𝑧1:𝑛⊤is\nL(𝑍; 𝜃)= ∑︁\n0≤𝑖<𝑗≤𝑛\n𝑅(𝑧1:𝑖⊤) 𝑗\n𝑘=𝑖+1 𝑞GFN (𝑧𝑘 |𝑧1:𝑘−1)𝑞GFN (⊤|𝑧1: 𝑗)\nlog\n𝑅(𝑧1: 𝑗⊤)𝑞GFN (⊤|𝑧1:𝑖)\n2\n, (3)\nFor sequence generation tasks, the SubTB objective is equivalent to the path consistency objec-\ntive (Nachum et al., 2017) in max-entropy RL (Haarnoja et al., 2017), which has been previously\nused in the context of text generation (Guo et al., 2021). See Appendix A.2 for further discussion.\nTraining policy. As the objective in Eq. 3 can be minimized to 0 for all trajectories 𝜏simultaneously\ngiven enough model capacity, we can use trajectories sampled from any full-support distribution\n(training policy) to perform gradient descent on L(𝜏; 𝜃)with respect to 𝜃. As the space we are\nsampling from is combinatorially large, it is important to have a training policy that can efficiently\nexplore Z. To this end, we compose the mini-batch during training using trajectories from three\nsources: (1) the policy 𝑞GFN, (2) a tempered version of the current policy 𝑞GFN and (3) a replay\nbuffer storing past trajectories. Replay buffers have been shown to be quite effective in improving\nGFlowNet training (Jain et al., 2022; Deleu et al., 2022; Shen et al., 2023).\nParametrization, amortization, and generalization. To sample the latent sequence 𝑍 from the\nposterior defined in Eq. 2, we parametrize the GFlowNet policy as an autoregressive language\nmodel that samples the latent 𝑍 one token at a time from left to right. By setting the reward\n𝑅(𝑍)= 𝑝LM (𝑋𝑍𝑌)∝𝑝LM (𝑍 |𝑋,𝑌), we learn a sampler for the posterior at convergence.\n5\nPublished as a conference paper at ICLR 2024\nAs illustrated in Fig. 1, depending on the task, we can condition the GFlowNet policy on either 𝑋\nor 𝑋,𝑌. In cases such as reasoning (§3.2), where there is only a single correct 𝑌 for each 𝑋and we\ninterested in predicting 𝑌 for unseen 𝑋at test time, we can simply condition on 𝑋. In this case, the\nGFlowNet policy is simply a language model that generates 𝑍as a continuation of 𝑋. To be precise,\nwe initialize 𝑞GFN as a copy of 𝑝LM that is conditioned on the prefix 𝑋, and then fine-tune2 it with a\nGFlowNet objective. With this view, sampling 𝑍is an inverse problem: we need to infer 𝑍given a\n(conditional) prior 𝑝LM (𝑍 |𝑋)and an observation 𝑌 under likelihood model 𝑝LM (𝑌 |𝑋𝑍).\nAllowing the GFlowNet policy to explicitly take 𝑋as input amortizes the sampling procedure and\nallows generalization to unseen 𝑋. In this sense, the GFlowNet is a Bayesian model (akin to a\nLM cascade (Dohan et al., 2022) or deep language network (Sordoni et al., 2023)), in which 𝑍\nare conditionally sampled ‘parameters’ that transform 𝑋 into 𝑌. To predict the 𝑌 for an unseen 𝑋,\none performs Bayesian model averaging by drawing samples of 𝑍 from 𝑞GFN (𝑍 |𝑋)followed by\nsampling from 𝑝LM (𝑌 |𝑋𝑍).\nIn tasks such as infilling (§4.2), however, the mapping from 𝑋to 𝑌 is one-to-many and 𝑌 is available\nat test-time. Here, we are interested in 𝑍 itself, rather than using it as an intermediate variable en\nroute to generating 𝑌. The GFlowNet policy thus has to be conditioned on both 𝑋and 𝑌. To achieve\nthis, the policy is conditioned on a prompt that contains both 𝑋and 𝑌 (for example, see Appendix C).\n4 EMPIRICAL RESULTS\nWe first validate GFlowNet fine-tuning on text generation, where we seek to find likely sentence\ncontinuation given a prompt (§4.1) or fill in a missing sentence in a story (§4.2). Then, we study\nreasoning tasks that benefit from chain-of-thought reasoning (§4.3) and external tool use (§4.4).\n4.1 SENTENCE CONTINUATION\nTask description. A natural application for au-\ntoregressive language models is that of sequence\ncontinuation: given a prompt, the model should\ngenerate a high-likelihood completion. In ap-\nplications such as creative writing, we would\nlike the continuations to be semantically diverse\nwhile still having a high likelihood under the\nlanguage model. To demonstrate the benefits of\nGFlowNet fine-tuning, we consider the task of\nsampling the next sentence following a prompt.\nSampling autoregressively from the LM until\na “.” token is reached is unlikely to produce\nsamples that have a high likelihood because the\ndistribution over sentences has a fat tail. Exist-\ning approaches to generate sequence continua-\ntions include beam search and its variations (Vi-\njayakumar et al., 2018; Shao et al., 2017), top-𝑘\n0.675 0.700 0.725 0.750 0.775 0.800\nSentence diversity\nFigure 3: Maximum log-likelihood and diversity of con-\ntinuations sampled for fixed prompts. GFlowNet fine-\ntuning (★) samples higher log-likelihood sentences while\nmaintaining more sample diversity than the baselines (•\nand ---), even when they are given 5×the compute.\nsampling (Fan et al., 2018), nucleus sampling\n(Holtzman et al., 2019), tempered autoregressive sampling, and fine-tuning using importance sampling\n(Shih et al., 2023), among others. While useful, most of these methods are ultimately hand-crafted\nheuristics that leave room for improvement. Furthermore, some of these methods (e.g., beam search)\ninvolve a computationally expensive search procedure, compared to a single pass of a learned in-\nference model that amortizes over prompts. Our GFlowNet policy autoregressively samples the\nsequence until a period is sampled, indicating the end of the sentence. Given prompts 𝑋, the LM is\nfine-tuned to generate the continuations 𝑍from the tempered posterior by being trained with reward\n𝑅(𝑍)= 𝑝LM (𝑍|𝑋)1\n𝑇 . When 𝑇= 1, the GFlowNet will trivially sample proportional to 𝑝LM without\nany fine-tuning, so we consider 0 <𝑇 <1 to focus on the likely continuations.\nWe consider a dataset of prompts from OpenWebText (Gokaslan et al., 2019) with a 1.5B param\nGPT-2 XL (Radford et al., 2019) as the base model. We draw 8 samples from the fine-tuned\nmodel conditioned on a fixed prompt, consider the maximum-likelihood sample under the LM,\nand report the average over the dataset of prompts. To measure the semantic diversity of the\n2We use LoRA (Hu et al., 2022) instead of full fine-tuning for hardware efficiency in all experiments.\nMaximum sentence log-likelihood\n10\n15\nDiverse beam\n(5X compute)\nT = 0.8\nDiverse beam\nGFlowet fine-tuning\nT = 0.95\n20\nSampling\nw/ T = 0.8\n25\nGreedy\nNucleus Sampling\nw/ T = 1.0\n30\n6\nPublished as a conference paper at ICLR 2024\nsamples, we compute the mean pairwise cosine distance between the embeddings (from a pretrained\nencoder (Reimers & Gurevych, 2019)) of the generated samples and average it over the dataset. We\ncompare to baselines that are commonly used for producing continuations from LMs at inference\ntime (beam search, diverse beam search, nucleus sampling, autoregressive sampling, tempered\nautoregressive sampling, and greedy generation).\nResults. Quantitative results are reported in Fig. 3 and empirical samples are shown in Appendix B.\nAt lower temperatures, our method excels in generating high-likelihood sentences, outperforming the\nleading baseline, diverse beam search. If we increase the number of beams (and therefore compute)\nto 5×the number of samples produced by the GFlowNet, our performance remains comparable.\nNevertheless, even in this scenario, the GFlowNet’s generated samples exhibit notably higher diversity\ncompared to diverse beam search and are on par with the best diversity-scoring benchmarks.\n4.2 INFILLING STORIES\nTask description. Next, we consider the story infilling task, a special case of the general infilling\nproblem (§3.1), where given the beginning 𝑋and end 𝑌 of a story, the goal is to generate the middle\nof the story 𝑍 (Zhu et al., 2019). This is challenging for a language model sampled left to right\nsince continuations 𝑍 conditioned only on 𝑋 might be incompatible with the ending 𝑌. We use\nthe ROCStories corpus (Mostafazadeh et al., 2016), a dataset of short stories containing exactly 5\nsentences each. Given the first 3 sentences and the last sentence, the goal is to generate the fourth\nsentence, which often involves a turning point in the story and is thus challenging to fill in.\nAs we expect the base model to contain the required knowledge, for this task we use a GPT-2 Large\nmodel (Radford et al., 2019) fine-tuned on the entire ROCStories training set as the base model.\nFor evaluating the approach, we consider 900 samples from the dataset as training data to learn\n𝑞GFN (𝑍|𝑋,𝑌)and evaluate the similarity of the generated infills on a dataset of 100 unseen stories.\nAlong with the GFlowNet-fine-tuned model, we also consider two baselines: prompting the model to\ninfill the story and supervised fine-tuning on the same data. Further details are in Appendix C.\nResults. To measure the similarity of the gen-\nerated infills with the reference infills available\nin the dataset, we compute BERTScore (Zhang\net al., 2020b), with DeBERTa (He et al., 2021)\nTable 2: Evaluation of the generated infills.\nMethod BERTScore BLEU-4 GLEU-4 GPT4Eval\nPrompting 0.081 ±0.009 1.3 ±0.5 3.2 ±0.1 2.4\nSupervised fine-tuning 0.094 ±0.007 1.6 ±0.8 3.7 ±0.4 2.7\nGFlowNet fine-tuning 0.184 ±0.004 2.1 ±0.2 4.2 ±0.7 3.4\n– which is correlated with human judgments –\nalong with BLEU-4 (Papineni et al., 2002) and GLEU-4 (better suited for sentences; Wu et al., 2016)\nmetrics. Additionally, we also evaluate each method using GPT-4 as a judge. From our results\nsummarized in Table 2, we observe that the infills generated by the model with GFlowNet fine-tuning\nare closer to the reference infills in the dataset than the baselines. By sampling from 𝑝LM (𝑍|𝑋,𝑌),\nthe GFlowNet is able to account for the ending while generating the infill, resulting in infills that link\nthe beginning and the end of the story coherently. For further analysis and details see Appendix C.\n4.3 SUBJECTIVITY CLASSIFICATION\nTask description. SUBJ (Pang & Lee, 2004)\nis a binary classification dataset for natural lan-\nguage understanding. It is a collection of movie\nreviews in which each review is labeled as objec-\ntive, meaning that it references facts about the\nmovie, or subjective, meaning that it expresses\nan opinion of the reviewer (see Table D.1 for ex-\nFew-shot prompting 61.3 ±6.2 61.8 ±5.4 65.8 ±10.5\namples). Given an unlabeled review, the model\nSupervised fine-tuning 64.3 ±2.8 69.1 ±0.8 89.7 ±0.4\nmust predict whether it is objective or subjective.\nGFlowNet fine-tuning 71.4 ±1.8 81.1 ±0.4 87.7 ±2.2\nWhile supervised fine-tuning on the full dataset\n+ Supervised fine-tuning 75.2 ±1.8 78.7 ±1.6 89.9 ±0.2\ncan achieve high test accuracy, we are interested in the low-data regime where we only have tens of\nlabeled examples. We use the same instruction-tuned GPT-J 6B variant as in §2 for this experiment.\nWithout any demonstrations, the model struggles with this task using the prompt in Table D.2 and\nachieves only 51.7% zero-shot accuracy.\nThis task is hard likely because it requires a latent reasoning step. A review could be considered\nobjective because it analyzes the plot or facts about the movie, or it could be subjective because it\nexpresses a personal opinion or makes a judgment. We denote the review 𝑋, the predicted subjectivity\n𝑌, and the latent reason 𝑍. Then, we GFlowNet-fine-tune the LLM 𝑞GFN (𝑍 |𝑋), initialized with the\nTable 3: Test accuracy (%) on SUBJ using an instruct-\nfine-tuned GPT-J 6B.\nMethod Test accuracy (%) ↑\nZero-shot prompting 51.7\nTraining samples\n10 20 50\n7\nPublished as a conference paper at ICLR 2024\nMethod Number of Operands\nIn-distribution OOD\n3 4 5\n𝑘-shot CoT 𝑘= 0 10.2 6.4 3.2\n𝑘= 3 15.8 ±3.1 11 ±1.7 5.4 ±0.2\n𝑘= 5 20.4 ±10.4 17.6 ±0.6 6.6 ±1.1\n𝑘= 10 26.5 ±1.4 15.2 ±1.7 8.9 ±1.9\n𝑘= 20 35.5 ±1.9 21 ±1.4 10.5 ±0.9\nSupervised fine-tuning 72.1 ±1.3 19.6 ±2.2 12.8 ±5.7\nPPO 30.6 ±4.1 13.7 ±4.1 5.6 ±3.1\nGFlowNet fine-tuning 95.2 ±1.3 75.4 ±2.9 40.7 ±9.1\nbase model 𝑝LM to match the Bayesian posterior over rationales in Eq. 2. At test time, 𝑞GFN (𝑍 |𝑋)\ngenerates 10 latent rationales (𝑍’s) for an unseen 𝑋. The LLM 𝑝LM then autoregressively samples\nfrom 𝑝LM (𝑌 |𝑋𝑍)to produce 10 answers, the majority vote of which becomes the final prediction.\nThis posterior inference corresponds to the E-step in the EM algorithm, where the posterior 𝑝LM (𝑍 |\n𝑋,𝑌)is defined in Eq. 2. Further, as described in §3.2, we can take an M-step by updating 𝑝LM to\nmaximize log 𝑝LM (𝑋𝑍𝑌)over a collection of 𝑍’s sampled from the amortized posterior 𝑞GFN. This\nis equivalent to applying supervised fine-tuning after GFlowNet fine-tuning.\nResults. We present few-shot prompting and supervised fine-tuning with LoRA as baselines. In\nfew-shot prompting, we prepend 0, 10, 20, or 50 training examples to each test example using the\nprompt shown in Table D.2. We randomly shuffle few-shot demonstrations and report the mean and\nvariance in Table 3. In supervised fine-tuning, we directly maximize log 𝑝LM (𝑌 |𝑋)over the same\n10, 20, or 50 (𝑋,𝑌)pairs. The variance is over model initialization and batch order. All entries except\nzero-shot prompting are aggregated over 3 random seeds. See Appendix D for experiment details.\nGFlowNet fine-tuning consistently outperforms supervised fine-tuning in the low-data regime, as\nshown in Table 3. In some cases, performing supervised fine-tuning on top, which corresponds to\nrunning one step of the EM algorithm, further improves the performance.\n4.4 SOLVING ARITHMETIC PROBLEMS STEP BY STEP\nTask description. Arithmetic reasoning is a\nfitting benchmark to evaluate reasoning abilities\nof large language models as it requires multi-\nTable 4: Test accuracy (%) on an integer arithmetic task\nwith addition and subtraction using a GPT-J 6B model.\nTraining data only include samples with 3 or 4 operands.\nstep reasoning and correctness is easy to evalu-\nate (Cobbe et al., 2021). While the distribution\nof pretraining and fine-tuning data (Magister\net al., 2023; Lee et al., 2023; Luo et al., 2023)\nand prompting choices (Imani et al., 2023) play\na critical role in their arithmetic abilities, LLMs\nare susceptible to poor generalization by learn-\ning ‘shortcuts’ to reasoning (Dziri et al., 2023).\nWe consider a simple integer arithmetic task\n(Fig. 1), with a general pretrained base model,\nrather than a one pretrained on mathematical\ntasks (Jelassi et al., 2023). To avoid the pitfalls of symbolic calculations with language models, we\nadopt the tool use setting (Schick et al., 2023), where the model is equipped with a calculator that can\nperform parts of the computation, implemented as in Cobbe et al. (2021): when the model outputs\n‘\n=’ the expression preceding it is evaluated and appended to the sequence. To prevent the model from\n“cheating”, we limit the calculator to evaluate only two-term expressions. Consequently, reasoning\nhere involves learning to plan using a tool with limited capabilities (Hao et al., 2023).\nFor training, we use a synthetic dataset of arithmetic expressions, limited to addition and subtraction.\nFollowing Zelikman et al. (2022), we use a small set of 50 demonstrations (𝑋,𝑍,𝑌)to seed the replay\nbuffer in addition to 1000 examples (𝑋,𝑌). We use the same instruction-tuned GPT-J as in §4.3 as\nthe base model. Further details are in Appendix E. We report the accuracy on two types of examples:\n(1) unseen in-distribution expressions (3 or 4 operands) and (2) longer out-of-distribution expressions\n(5 operands). As baselines, we consider zero-shot chain-of-thought prompting, 𝑘-shot prompting,\nsupervised fine-tuning on the tool use sequences, and fine-tuning with PPO (Schulman et al., 2017).\nFor all methods, we enable tool use and limit the model to generate only numbers and operators.\nResults. From the results summarized in Table 4, the base model performs poorly even with chain-\nof-thought prompts. Including examples in context improves the performance considerably, with\nmonotonic improvements as the number of examples increases. Supervised fine-tuning improves the\nperformance significantly on the in-distribution examples, but the model still struggles to generalize\non the out-of-distribution examples. Fine-tuning with PPO results also yields poor performance,\ncaused in part by the poor calibration of the base reward model, i.e. it cannot distinguish good\nrationales from bad ones. Even though the sequences generated with PPO (illustrated in Appendix E)\nhave high rewards, they are spurious and do not even define valid calls to the tool.\nSuch overoptimization to a misspecified reward is a widely noted issue in LLMs trained with\nRL (Gao et al., 2023). On the other hand, by matching the entire distribution, GFlowNet fine-tuning\navoids collapsing to a single mode of the reward, thereby being robust to the misspecification of\n8\nPublished as a conference paper at ICLR 2024\nthe reward (Eysenbach & Levine, 2022) and achieving significantly better performance on in and\nout-of-distribution examples. See Appendix E for additional results and analysis.\n5 FURTHER RELATED WORK\nSampling from intractable marginals. Beyond the approximations mentioned in §3.1, sampling\nfrom intractable posterior distributions given by pretrained models for tasks such as infilling and\nconstrained generation has been an object of study. Miao et al. (2019); Zhang et al. (2020a) use\nMCMC for these problems, Malkin et al. (2021) used a variable-neighborhood ascent for finding\nmodes, and a sequential Monte Carlo approach was recently proposed by Lew et al. (2023). Others\nhave studied the problem with masked language models, using them to perform variants of Gibbs\nsampling (Wang & Cho, 2019; Goyal et al., 2022; Yamakoshi et al., 2022) and recovering marginal\ndistributions over small sets of tokens (Torroba Hennigen & Kim, 2023).\nGFlowNets. GFlowNets (Bengio et al., 2021) were originally proposed to learn policies for sam-\npling discrete compositional objects from an unnormalized reward distribution, motivated by the\nneed to sample diverse high-reward objects in scientific discovery (Jain et al., 2023), in particular, for\nbiological sequence generation (Jain et al., 2022). The interpretation of GFlowNets as variational\ninference algorithms (Malkin et al., 2023; Zimmermann et al., 2023) makes them appropriate for\nsampling Bayesian posterior distributions over structured objects (e.g., Deleu et al., 2022; 2023; van\nKrieken et al., 2023; Hu et al., 2023).\nChain-of-thought reasoning in LLMs. In recent work on classification and completion with\nlanguage models, the latent reasoning chain 𝑍, in the notation of §3.1, is called a ‘chain of thought’\n(Wei et al., 2022). The chain of thought is typically generated by conditioning the language model on\n𝑋with the use of specialized demonstrations or prompts (Kojima et al., 2022), with no guarantee of\nsampling the posterior accurately. Related to our Bayesian formulation, Wang et al. (2023b) noted\nthat appropriately aggregating the conclusions 𝑌 from several latent chains 𝑍 improves predictive\nperformance. In Xu et al. (2023); Zhou et al. (2022), a posterior over latent token sequences is\nsampled using MCMC, while Zelikman et al. (2022) propose fine-tuning on successful (high-reward,\nin our language) chains of thought, which achieves reward maximization but gives no guarantee of\ndiversity. In concurrent work, Phan et al. (2023) use MCMC to sample chains-of-thought in problems\nwith binary feedback. We expect these methods to generalize poorly to difficult exploration problems,\nwhile GFlowNet fine-tuning takes advantage of generalizable structure in the posterior and has a goal\nof sampling the full posterior over latent reasoning chains.\n6 CONCLUSION\nThe knowledge compressed in LLMs is crucial for tasks such as infilling and constrained generation,\nbut querying this knowledge involves sampling from intractable posterior distributions. We propose\nto use GFlowNet objectives to train LLMs to sample from such posterior distributions. Empirical\nresults show that GFlowNet fine-tuning finds a better fidelity-diversity trade-off for text generation\nand also improves sample efficiency and generalization on downstream tasks compared to maximum-\nlikelihood training or reward-maximizing policy optimization. As an amortized inference algorithm,\nour method converts computation into better test-time performance without additional data.\nFuture work should investigate transfer and generalization across tasks, in particular, building a\n‘universal reasoner’ as a model 𝑞(𝑍 |𝑋)shared between 𝑋 from different tasks, as was recently\nconsidered by Wang et al. (2023a). One should investigate the benefit of using a better knowledge\nmodel, e.g., a more capable base LLM, as a starting point for GFlowNet fine-tuning. The ability to\ndraw multiple samples from a GFlowNet can also be used to quantify epistemic uncertainty. Finally,\nwe adopt the GFlowNet formalisms with the perspective of generalizing to latent variables 𝑍 with\na richer generative process than left-to-right sampling. We hope that the GFlowNet paradigm will\nenable more flexible reasoning with LLMs in the future: extending probabilistic programming with\nlanguage variables (Beurer-Kellner et al., 2023), using structured chains of thought (Yao et al., 2023;\nBesta et al., 2024), and extending to program synthesis and planning with world models.\nLimitations. Due to resource constraints, our experiments use models up to 6B parameters, but we\nexpect the conclusions to hold for larger models. In fact, our method can potentially benefit larger\nmodels more: it is harder to optimize a larger model with maximizing objectives on a small amount of\ndata. As with any on-policy method, exploration, especially in problems with more complex latents,\nremains an open problem. Furthermore, our method improves inference but not the knowledge in\nthe LM. Issues such as hallucination or miscalibration, which are closely related to the knowledge\nrepresentation, are thus not addressed.\n9\nPublished as a conference paper at ICLR 2024\nETHICS STATEMENT\nWhile we foresee no immediate negative societal consequences of our work, we hope that future\nresearchers who build upon it will, as we have, bear in mind the potential of LLMs – and in particular\nof human-like reasoning in LLMs – to be used both for good and for harm.\nResearch areas in safe and explainable AI that can benefit from GFlowNet fine-tuning include (1)\ninterpretability of LLMs’ reasoning processes and (2) fine-tuning with human feedback or an external\nreward, where diverse sampling can help prevent ‘reward hacking’ and overfitting to a misspecified\ntarget.\nREPRODUCIBILITY\nWe discuss the details of the proposed algorithms in §3.3 and provide all the implementation details\nand hyperparameters for the experiments in the main paper and appendix. Code for our experiments\nis available at https://github.com/GFNOrg/gfn-lm-tuning.\nACKNOWLEDGMENTS\nThe authors are grateful to Bonaventure Dossou and Salem Lahlou for their help in the early stages\nof this project. We also thank Robert Hawkins, Arian Hosseini, Zhen Wang, and Anirudh Goyal for\nvaluable discussions and suggestions of related work.\nGL acknowledges funding from CIFAR, Samsung, and a Canada Research Chair in Neural Computa-\ntion and Interfacing.\nYB acknowledges funding from CIFAR, NSERC, IBM, Intel, Genentech, and Samsung.\nThe research was enabled in part by computational resources provided by the Digital Research\nAlliance of Canada (https://alliancecan.ca), Mila (https://mila.quebec), and\nNVIDIA.\nREFERENCES\nMatthew J. Beal. Variational algorithms for approximate Bayesian inference, 2003. URL https:\n//cse.buffalo.edu/faculty/mbeal/papers/beal03.pdf.\nFrank Benford. The law of anomalous numbers. Proceedings of the American Philosophical Society,\n78(4):551–572, 1938. ISSN 0003049X. URL http://www.jstor.org/stable/984802.\nEmmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow\nnetwork based generative models for non-iterative diverse candidate generation. Neural Information\nProcessing Systems (NeurIPS), 2021.\nYoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio.\nGFlowNet foundations. Journal of Machine Learning Research, (24):1–76, 2023.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda,\nTomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of\nthoughts: Solving elaborate problems with large language models. Association for the Advancement\nof Artificial Intelligence (AAAI), 2024.\nLuca Beurer-Kellner, Marc Fischer, and Martin Vechev. Prompting is programming: A query\nlanguage for large language models. Proceedings of the ACM on Programming Languages, 7, jun\n2023.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve\nmath word problems. arXiv preprint arXiv:2110.14168, 2021.\nTristan Deleu, Ant´ onio G´ ois, Chris Emezue, Mansi Rankawat, Simon Lacoste-Julien, Stefan Bauer,\nand Yoshua Bengio. Bayesian structure learning with generative flow networks. Uncertainty in\nArtificial Intelligence (UAI), 2022.\nTristan Deleu, Mizu Nishikawa-Toomey, Jithendaraa Subramanian, Nikolay Malkin, Laurent Charlin,\nand Yoshua Bengio. Joint Bayesian inference of graphical structure and parameters with a single\ngenerative flow network. Neural Information Processing Systems (NeurIPS), 2023.\n10\nPublished as a conference paper at ICLR 2024\nA. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the\nEM algorithm. Journal of the Royal Statistical Society B, 39(1):1–38, 1977.\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes,\nYuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-Dickstein, Kevin Murphy, and\nCharles Sutton. Language model cascades. arXiv preprint arXiv:2207.10342, 2022.\nChris Donahue, Mina Lee, and Percy Liang. Enabling language models to fill in the blanks. In\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.\n2492–2501, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nacl-main.225. URL https://aclanthology.org/2020.acl-main.225.\nNouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West,\nChandra Bhagavatula, Ronan Le Bras, Jena D Hwang, et al. Faith and fate: Limits of transformers\non compositionality. Neural Information Processing Systems (NeurIPS), 2023.\nBenjamin Eysenbach and Sergey Levine. Maximum entropy RL (provably) solves some robust RL\nproblems. International Conference on Learning Representations (ICLR), 2022.\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 889–898, Melbourne, Australia, July 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/P18-1082. URL https://aclanthology.org/P18-1082.\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization.\nInternational Conference on Machine Learning (ICML), 2023.\nSamuel J. Gershman and Noah D. Goodman. Cognitive Science, 36, 2014.\nAmortized inference in probabilistic reasoning.\nAaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http:\n//Skylion007.github.io/OpenWebTextCorpus, 2019.\nKartik Goyal, Chris Dyer, and Taylor Berg-Kirkpatrick. Exposing the implicit energy networks\nbehind masked language models via Metropolis-Hastings. International Conference on Learning\nRepresentations (ICLR), 2022.\nHan Guo, Bowen Tan, Zhengzhong Liu, Eric P. Xing, and Zhiting Hu. Efficient (soft) Q-learning for\ntext generation with limited good data. arXiv preprint arXiv:2106.07704, 2021.\nTuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with\ndeep energy-based policies. International Conference on Machine Learning (ICML), 2017.\nShibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. Reasoning\nwith language model is planning with world model. In Houda Bouamor, Juan Pino, and Kalika\nBali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pp. 8154–8173, Singapore, December 2023. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2023.emnlp-main.507. URL https://aclanthology.org/2023.\nemnlp-main.507.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. DeBERTa: Decoding-enchanced\nBERT with disentangled attention. International Conference on Learning Representations (ICLR),\n2021.\nChris Hokamp and Qun Liu. Lexically constrained decoding for sequence generation using grid\nbeam search. In Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 1535–1546, Vancouver, Canada, July 2017. Association\nfor Computational Linguistics. doi: 10.18653/v1/P17-1141. URL https://aclanthology.\norg/P17-1141.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration. International Conference on Learning Representations (ICLR), 2019.\n11\nPublished as a conference paper at ICLR 2024\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language models. International Conference on Learning\nRepresentations (ICLR), 2022.\nEdward J Hu, Nikolay Malkin, Moksh Jain, Katie Everett, Alexandros Graikos, and Yoshua Bengio.\nGFlowNet-EM for learning compositional latent variable models. International Conference on\nMachine Learning (ICML), 2023.\nJ. Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin\nVan Durme. Improved lexically constrained decoding for translation and monolingual rewriting.\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),\npp. 839–850, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:\n10.18653/v1/N19-1090. URL https://aclanthology.org/N19-1090.\nShima Imani, Liang Du, and Harsh Shrivastava. MathPrompter: Mathematical reasoning us-\ning large language models. In Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 5: Industry Track), pp. 37–42, Toronto, Canada, July\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-industry.4. URL\nhttps://aclanthology.org/2023.acl-industry.4.\nMoksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure F.P.\nDossou, Chanakya Ekbote, Jie Fu, Tianyu Zhang, Micheal Kilgour, Dinghuai Zhang, Lena Simine,\nPayel Das, and Yoshua Bengio. Biological sequence design with GFlowNets. International\nConference on Machine Learning (ICML), 2022.\nMoksh Jain, Tristan Deleu, Jason Hartford, Cheng-Hao Liu, Alex Hernandez-Garcia, and Yoshua\nBengio. GFlowNets for AI-driven scientific discovery. Digital Discovery, 2(3):557–577, 2023.\nSamy Jelassi, St´ ephane d’Ascoli, Carles Domingo-Enrich, Yuhuai Wu, Yuanzhi Li, and Franc¸ois\nCharton. Length generalization in arithmetic transformers. arXiv preprint arXiv:2306.15400,\n2023.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Language\nmodels are zero-shot reasoners. Neural Information Processing Systems (NeurIPS), 2022.\nDaphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT\npress, 2009.\nNayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, and Dimitris Papailiopoulos.\nTeaching arithmetic to small transformers. arXiv preprint arXiv:2307.03381, 2023.\nAlexander K. Lew, Tan Zhi-Xuan, Gabriel Grand, and Vikash K. Mansinghka. Sequential Monte Carlo\nsteering of large language models using probabilistic programs. arXiv preprint arXiv:2306.03081,\n2023.\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto,\nLuke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as op-\ntimization. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 12286–12312, Toronto, Canada, July 2023. Asso-\nciation for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.687. URL https:\n//aclanthology.org/2023.acl-long.687.\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith,\nand Yejin Choi. DExperts: Decoding-time controlled text generation with experts and anti-experts.\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),\npp. 6691–6706, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/\nv1/2021.acl-long.522. URL https://aclanthology.org/2021.acl-long.522.\nDayiheng Liu, Jie Fu, Pengfei Liu, and Jiancheng Lv. TIGS: An inference algorithm for text infilling\nwith gradient search. In Proceedings of the 57th Annual Meeting of the Association for Compu-\ntational Linguistics, pp. 4146–4156, Florence, Italy, July 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/P19-1406. URL https://aclanthology.org/P19-1406.\n12\nPublished as a conference paper at ICLR 2024\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-Eval: NLG\nevaluation using GPT-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023.\nSidi Lu, Tao Meng, and Nanyun Peng. Insnet: An efficient, flexible, and performant insertion-based\ntext generation model. Neural Information Processing Systems (NeurIPS), 2022a.\nXiming Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan\nLe Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith, and Yejin Choi. Neu-\nroLogic a*esque decoding: Constrained text generation with lookahead heuristics. In Proceed-\nings of the 2022 Conference of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, pp. 780–799, Seattle, United States, July\n2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.57. URL\nhttps://aclanthology.org/2022.naacl-main.57.\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-Guang Lou, Chongyang Tao, Xiubo Geng,\nQingwei Lin, Shifeng Chen, and Dongmei Zhang. WizardMath: Empowering mathematical\nreasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308:09583,\n2023.\nKanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, An-\ndrei Cristian Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning GFlowNets from\npartial episodes for improved convergence and stability. International Conference on Machine\nLearning (ICML), 2023.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn.\nTeaching small language models to reason. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2: Short Papers), pp. 1773–1781, Toronto,\nCanada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-short.\n151. URL https://aclanthology.org/2023.acl-short.151.\nNikolay Malkin, Sameera Lanka, Pranav Goel, and Nebojsa Jojic. Studying word order through\niterative shuffling. In Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 10351–10366, Online and Punta Cana, Dominican Republic, November\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.809. URL\nhttps://aclanthology.org/2021.emnlp-main.809.\nNikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance:\nImproved credit assignment in GFlowNets. Neural Information Processing Systems (NeurIPS),\n2022a.\nNikolay Malkin, Zhen Wang, and Nebojsa Jojic. Coherence boosting: When your pretrained language\nmodel is not paying enough attention. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pp. 8214–8236, Dublin, Ireland, May\n2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.565. URL\nhttps://aclanthology.org/2022.acl-long.565.\nNikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai Zhang,\nand Yoshua Bengio. GFlowNets and variational inference. International Conference on Learning\nRepresentations (ICLR), 2023.\nTao Meng, Sidi Lu, Nanyun Peng, and Kai-Wei Chang. Controllable text generation with neurally-\ndecomposed oracle. Neural Information Processing Systems (NeurIPS), 2022.\nNing Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li. CGMH: Constrained sentence generation by\nMetropolis-Hastings sampling. Association for the Advancement of Artificial Intelligence (AAAI),\n2019.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Van-\nderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper under-\nstanding of commonsense stories. In Proceedings of the 2016 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, pp.\n839–849, San Diego, California, June 2016. Association for Computational Linguistics. doi:\n10.18653/v1/N16-1098. URL https://aclanthology.org/N16-1098.\n13\nPublished as a conference paper at ICLR 2024\nOfir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between\nvalue and policy based reinforcement learning. Neural Information Processing Systems (NIPS),\n2017.\nBo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summa-\nrization based on minimum cuts. In Proceedings of the 42nd Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL-04), pp. 271–278, Barcelona, Spain, July 2004. doi:\n10.3115/1218955.1218990. URL https://aclanthology.org/P04-1035.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for auto-\nmatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the\nAssociation for Computational Linguistics, pp. 311–318, Philadelphia, Pennsylvania, USA,\nJuly 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL\nhttps://aclanthology.org/P02-1040.\nFabio Petroni, Tim Rockt¨ aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), pp. 2463–2473, Hong Kong,\nChina, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250.\nURL https://aclanthology.org/D19-1250.\nDu Phan, Matthew Douglas Hoffman, Sholto Douglas, Tuan Anh Le, Aaron T Parisi, Pavel Sountsov,\nCharles Sutton, Sharad Vikram, Rif A Saurous, et al. Training chain-of-thought via latent-variable\ninference. Neural Information Processing Systems (NeurIPS), 2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pp. 3982–3992, Hong Kong, China, November 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/D19-1410.\nAlex Renda, Aspen K. Hopkins, and Michael Carbin. Can LLMs generate random numbers?\nevaluating LLM sampling in controlled domains, 2023. URL http://people.csail.mit.\nedu/renda/llm-sampling-paper.\nTimo Schick and Hinrich Sch¨ utze. It’s not just size that matters: Small language models are also\nfew-shot learners. In Proceedings of the 2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, pp. 2339–2352,\nOnline, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.\n185. URL https://aclanthology.org/2021.naacl-main.185.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess` ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. Neural Information Processing Systems (NeurIPS), 2023.\nAllen Schmaltz, Alexander M. Rush, and Stuart Shieber. Word ordering without syntax. In Pro-\nceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp.\n2319–2324, Austin, Texas, November 2016. Association for Computational Linguistics. doi:\n10.18653/v1/D16-1255. URL https://aclanthology.org/D16-1255.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nLei Sha. Gradient-guided unsupervised lexically constrained text generation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8692–\n8703, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nemnlp-main.701. URL https://aclanthology.org/2020.emnlp-main.701.\n14\nPublished as a conference paper at ICLR 2024\nYuanlong Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, and Ray Kurzweil.\nGenerating high-quality and informative conversation responses with sequence-to-sequence models.\nIn Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp.\n2210–2219, Copenhagen, Denmark, September 2017. Association for Computational Linguistics.\ndoi: 10.18653/v1/D17-1235. URL https://aclanthology.org/D17-1235.\nMax W Shen, Emmanuel Bengio, Ehsan Hajiramezanali, Andreas Loukas, Kyunghyun Cho, and\nTommaso Biancalani. Towards understanding and improving gflownet training. International\nConference on Machine Learning (ICML), 2023.\nAndy Shih, Dorsa Sadigh, and Stefano Ermon. Long horizon temperature scaling. International\nConference on Machine Learning (ICML), 2023.\nAlessandro Sordoni, Xingdi Yuan, Marc-Alexandre Cˆ ot´ e, Matheus Pereira, Adam Trischler, Ziang\nXiao, Arian Hosseini, Friederike Niedtner, and Nicolas Le Roux. Joint prompt optimization of\nstacked LLMs using variational inference. Neural Information Processing Systems (NeurIPS),\n2023.\nRaymond Hendy Susanto, Shamil Chollampatt, and Liling Tan. Lexically constrained neural ma-\nchine translation with Levenshtein transformer. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics, pp. 3536–3543, Online, July 2020. Asso-\nciation for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.325. URL https:\n//aclanthology.org/2020.acl-main.325.\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press,\nsecond edition, 2018. URL http://incompleteideas.net/book/the-book-2nd.\nhtml.\nChristoph Tillmann and Hermann Ney. Word Reordering and a Dynamic Programming Beam Search\nAlgorithm for Statistical Machine Translation. Computational Linguistics, 29(1):97–133, 03 2003.\nISSN 0891-2017. doi: 10.1162/089120103321337458. URL https://doi.org/10.1162/\n089120103321337458.\nLucas Torroba Hennigen and Yoon Kim. Deriving language models from masked language models. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume\n2: Short Papers), pp. 1149–1159, Toronto, Canada, July 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.acl-short.99. URL https://aclanthology.org/\n2023.acl-short.99.\nEmile van Krieken, Thiviyan Thanapalasingam, Jakub Tomczak, Frank van Harmelen, and Annette\nten Teije. A-NeSI: A scalable approximate method for probabilistic neurosymbolic inference.\nNeural Information Processing Systems (NeurIPS), 2023.\nAshwin K Vijayakumar, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee, David\nCrandall, and Dhruv Batra. Diverse beam search: Decoding diverse solutions from neural sequence\nmodels. Association for the Advancement of Artificial Intelligence (AAAI), 2018.\nLeandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan\nLambert, and Shengyi Huang. Trl: Transformer reinforcement learning. https://github.\ncom/huggingface/trl, 2020.\nAlex Wang and Kyunghyun Cho. BERT has a mouth, and it must speak: BERT as a Markov random\nfield language model. In Proceedings of the Workshop on Methods for Optimizing and Evaluating\nNeural Language Generation, pp. 30–36, Minneapolis, Minnesota, June 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/W19-2304. URL https://aclanthology.\norg/W19-2304.\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 billion parameter autoregressive language model.\nhttps://github.com/kingoflolz/mesh-transformer-jax, May 2021.\nPeiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, and Zhifang Sui.\nMaking large language models better reasoners with alignment. arXiv preprint arXiv:2309.02144,\n2023a.\n15\nPublished as a conference paper at ICLR 2024\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.\nInternational Conference on Learning Representations (ICLR), 2023b.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le,\nand Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. Neural\nInformation Processing Systems (NeurIPS), 2022.\nSean Welleck, Ilia Kulikov, Jaedeok Kim, Richard Yuanzhe Pang, and Kyunghyun Cho. Consistency\nof a recurrent language model with respect to incomplete decoding. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP), pp. 5553–5568,\nOnline, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nemnlp-main.448. URL https://aclanthology.org/2020.emnlp-main.448.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson,\nXiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex\nRudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s neural\nmachine translation system: Bridging the gap between human and machine translation. arXiv\npreprint arXiv:1609.08144, 2016.\nWeijia Xu, Andrzej Banburski-Fahey, and Nebojsa Jojic. Reprompting: Automated chain-of-thought\nprompt inference through Gibbs sampling. arXiv preprint arXiv:2305.09993, 2023.\nTakateru Yamakoshi, Thomas Griffiths, and Robert Hawkins. Probing BERT’s priors with serial\nreproduction chains. In Findings of the Association for Computational Linguistics: ACL 2022, pp.\n3977–3992, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/\nv1/2022.findings-acl.314. URL https://aclanthology.org/2022.findings-acl.\n314.\nKevin Yang and Dan Klein. FUDGE: Controlled text generation with future discriminators. In\nProceedings of the 2021 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, pp. 3511–3535, Online, June 2021.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.276. URL\nhttps://aclanthology.org/2021.naacl-main.276.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. Neural\nInformation Processing Systems (NeurIPS), 2023.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. STaR: Bootstrapping reasoning with\nreasoning. Neural Information Processing Systems (NeurIPS), 2022.\nMaosen Zhang, Nan Jiang, Lei Li, and Yexiang Xue. Language generation via combinatorial\nconstraint satisfaction: A tree search enhanced Monte-Carlo approach. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020, pp. 1286–1298, Online, November 2020a.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.115. URL\nhttps://aclanthology.org/2020.findings-emnlp.115.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. BERTScore:\nEvaluating text generation with BERT. International Conference on Learning Representations\n(ICLR), 2020b.\nHattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi.\nTeaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and\nJimmy Ba. Large language models are human-level prompt engineers. International Conference\non Learning Representations (ICLR), 2023.\nWanrong Zhu, Zhiting Hu, and Eric Xing. Text infilling. arXiv preprint arXiv:1901.00158, 2019.\nHeiko Zimmermann, Fredrik Lindsten, Jan-Willem van de Meent, and Christian A. Naesseth. A\nvariational perspective on generative flow networks. Transactions on Machine Learning Research\n(TMLR), 2023.\n16\nPublished as a conference paper at ICLR 2024\nA ADDITIONAL BACKGROUND\nA.1 GLOSSARY OF RL FOR LLMS\nWe provide definitions for key terms used throughout the paper, with a focus on their relevance to our\nsetting of fine-tuning large language models (LLMs).\nReinforcement learning. Reinforcement learning (RL) is a branch of machine learning concerned\nwith how agents should take actions in an environment to maximize cumulative rewards. In our\ncontext, RL is used to fine-tune the decision-making process of LLMs to improve their performance\non specific tasks. For a more comprehensive overview, we refer readers to Sutton & Barto (2018).\nPolicy. In RL, a policy is a strategy that defines the behavior of an agent by mapping states of\nthe environment to actions. In our setting, a policy dictates how the language model generates text\nsequences based on the current context and learned parameters.\nReward. A reward is a signal that evaluates the quality of an action taken by an agent in a particular\nstate. In the fine-tuning of LLMs, rewards are used to guide the model towards generating more\ndesirable text, such as more accurate predictions or more coherent continuations. More specifically, in\nthe context of GFlowNets, the reward corresponds to the unnormalized posterior probability, and the\nGFlowNet aims to match it by learning a policy that generates samples proportional to their reward.\nMatching a distribution. Matching a distribution involves training a model to approximate a target\nprobability distribution. In our work, this concept is applied to fine-tune LLMs so that their generated\ntext matches the desired characteristics, such as adhering to a particular style or content constraint.\nPolicy gradient methods. Policy gradient methods (such as PPO) are a subset of RL algorithms\nthat optimize a policy by computing gradients of the expected reward with respect to the policy\nparameters. In the context of LLMs, these methods are used to fine-tune the model’s parameters to\nincrease the likelihood of generating high-reward text sequences.\nA.2 LEARNING OBJECTIVE\nWe use the subtrajectory balance learning objective for training GFlowNets (Madan et al., 2023). In\nthe notation of that paper, with a forward policy 𝑃𝐹, backward policy 𝑃𝐵 and state flow function 𝐹,\nthe objective over a partial trajectory 𝜏= 𝑠𝑚 →···→𝑠𝑛 is defined as follows\nL𝑆𝑢𝑏𝑇𝐵(𝑍; 𝜃)= log\n𝐹(𝑠𝑚)𝑛−1\n𝑖=𝑚𝑃𝐹(𝑠𝑖+1 |𝑠𝑖)\n𝐹(𝑠𝑛)𝑛−1\n𝑖=𝑚𝑃𝐵(𝑠𝑖|𝑠𝑖+1)\n2\n(4)\nIn the case of autoregressive generation of a sequence of tokens in a fixed order (left-right), the\ngenerative process is a tree, so there is only a single path to each state, and each state has a single\nparent. Thus, 𝑃𝐵(𝑠|𝑠′)= 1 trivially. Additionally, since each state is a valid terminable state, we\ncan incorporate the modification to account for this from Deleu et al. (2022). Specifically, note\nthat at convergence we have 𝑅(𝑠⊤\n𝑛)= 𝐹(𝑠𝑛)𝑃𝐹(⊤ | 𝑠𝑛). Using this, we can simply substitute\n𝐹(𝑠𝑛)= 𝑅(𝑠⊤\n𝑛)/𝑃𝐹(⊤|𝑠𝑛)in Eq. (4). This allows us to avoid parameterizing a flow function\nseparately, reducing additional complexity. The only learned object is now the forward sampling\npolicy 𝑃𝐹, which we refer to as 𝑞𝐺𝐹𝑁. Summing this over all partial trajectories in a trajectory with\nequal weight (𝜆= 1), we get the final learning objective in Eq. (3).\nB SENTENCE CONTINUATION\nAdditional details. We choose sentences as the level of granularity for our sequence continuation\ntask because they are a natural unit of generation in language. Moreso than individual words,\nsentences are analogous to whole thoughts, reasoning steps, etc. because the compositional rules of\nsyntax operate at the level of sentences. Whereas the meaning of a word is often ambiguous and\ncontext-dependent, the meaning of a sentence tends to be more self-contained.\nA naive solution to the task is to simply sample autoregressively from the LM until a “.” token is\nreached, marking the end of the sentence. In practice, however, this is unlikely to produce samples\nthat have high likelihood because the distribution over sentences has a long tail: a vast number of\nsentences have small but non-zero probability, and collectively account for a substantial portion of the\ntotal probability mass. Instead, it would be desirable to sample from a low-temperature distribution\nover sentences, so that the distribution becomes more sparse and highly probable sentences are\n17\nPublished as a conference paper at ICLR 2024\n1\n𝑇 (5)\nsampled more often. However, this in itself is a difficult distribution to sample from, and it is\ndifferent from simply sampling autoregressively from the LM with a lower temperature applied to\nthe distributions over the next words. For instance, as the temperature approaches 0, the desired\ndistribution over sentences should converge to an argmax and produce the single most likely next\nsentence. However, if we try to apply the temperature to the distribution over the next words and\nsample autoregressively, then as the temperature approaches 0, the resulting policy will greedily\nconstruct a sentence by sequentially picking the most likely next word, which is unlikely to produce\nthe highest probability sentence.\nOur GFlowNet sampling policy parametrizes a distribution 𝑝GFN (𝑤𝑖+1 |𝑤1:𝑖)using the same archi-\ntecture as the LM 𝑝LM (𝑤𝑖+1 |𝑤1:𝑖), and at the start of training, it is initialized with the same weights.\nThe initial state of the GFlowNet consists of a prompt 𝑤1:𝑘 that conditions generation (i.e., the text\nwhose next sentence we are trying to sample). Each subsequent state 𝑤1:𝑘+𝑖 is an extension of this\ntext and is obtained by sampling autoregressively from 𝑝GFN. This process terminates when a period\n“.” is sampled, indicating the end of the next sentence. If a predefined maximum sentence length is\nreached, we force a “.” action to manually terminate the generation.\nThe GFlowNet policy is trained to sample sentences in proportion to their tempered probability under\nthe original LM given the prompt. Denote the length of the prompt by 𝑘, the length of the sampled\nsentence by 𝑚, and the temperature by 𝑇. Then, the reward is:\n𝑅(𝑤1:𝑘+𝑚)def\n= 𝑝LM (𝑤𝑘+1:𝑘+𝑚|𝑤1:𝑘)1\n𝑇 =\n𝑚\n𝑖=1\n𝑝LM (𝑤𝑘+𝑖|𝑤1:𝑘+𝑖−1)\nThe intuition behind this reward is that we are assuming that the LM can reliably assign a high\nlikelihood to high-probability sentences, that we wish to preferentially sample over low-probability\nones. If we were to set 𝑇= 1, then the solution for the GFlowNet would be to sample proportionally\nto 𝑝LM (which it is initialized to). The preferential sampling of high-probability sentences is therefore\nobtained by setting 0 <𝑇 <1.\nTo run our experiment, we obtained a dataset of 1000 prompts from OpenWebText (Gokaslan et al.,\n2019) that were each 1-3 sentences long, 50 of which were used for validation. Our LM consisted of\na pretrained 1.5B parameter GPT2-XL (Radford et al., 2019), and our GFlowNet was a copy of this\nmodel that was fine-tuned in a lower-dimensional space of 80M parameters using LoRA (Hu et al.,\n2022).\nAdditional results. We show examples of empirical next-sentence samples generated by our model\nin Table B.1 compared to baselines. The model was trained using a reward temperature of 0.875,\nwhich achieves a good balance between log-likelihood and diversity.\nC INFILLING STORIES\nAdditional details. See Table C.1 for training examples from the subset of the ROC Stories dataset\nused for the task. To condition the model on 𝑋and 𝑌, as well as for the prompting baseline, we use\nthe following prompt:\n\"Beginning: {X}\n End: {Y}\n Middle: \"\nAn assumption we make in this paper is that the base language model already contains the knowledge\nrequired for the task, and the goal is to perform inference over this knowledge. However, for this task,\nnone of the pretrained base models were good at assigning high likelihoods to plausible stories. Thus,\nwe fine-tuned GPT-2 Large model (Radford et al., 2019) on the entirety of the stories dataset and\nused this fine-tuned model as the reward model. This was done with full fine-tuning using the trl\nlibrary (von Werra et al., 2020). We trained for 20 epochs with a batch size of 64 and 32 gradient\naccumulation steps and a learning rate of 0.0005.\nWe detail the hyperparameters used for training GFlowNets in our experiments in Table C.2. During\ntraining, we sample (𝑋,𝑌)from the dataset and then sample (batch size) 𝑍s for every (𝑋,𝑌), before\nusing 𝑝LM (𝑋𝑍𝑌)as the reward. We use the replay buffer described in §3.3 and seed it with rationales\nfrom the dataset. We linearly anneal the reward temperature, the temperature of the behavior policy\nduring training, and the learning rate during warmup. For supervised fine-tuning, we use a batch\nsize of 256 and train for 10 epochs with a learning rate of 0.0001 with trl and LoRA. At test time,\nwe sample 1024 infills for each example in the test set from all the models at temperature 0.9, and\naverage over 10 such draws.\n18\nPublished as a conference paper at ICLR 2024\nTable B.1: Empirical examples for sequence continuation. Sentences generated from GFlowNet fine-tuning tend\nto be more reasonable than autoregressively generated samples from the LM with temperature 1.0 and tend to be\nmore diverse than samples generated from diverse beam search.\nInput Prompt: The matching campaign starts on Friday and will continue through\nmidnight on Monday.\nSampling w/ 𝑇= 1.0: (1) Participate with a fairly low $1 donation so we can motivate\nmore volunteers on Sunday afternoon.\n(2) However, the information regarding Cutler’s suspicious death\nmay not become widely known until early in the week.\nDiverse beam search: (1) If you are interested in participating in the matching campaign,\nyou can do so by clicking here.\n(2) There is no limit to the number of times you can enter.\nGFlowNet fine-tuning: (1) If you are interested in signing up you can do so here.\n(2) Please share.\nInput Prompt: I want hockey to become a huge thing in Houston.\nSampling w/ 𝑇= 1.0: (1) We’ll be here in Texas from late November till end March.\n(2) To that end, I’ve been following the Houston Aeros (Houston\nDynamo’s minor-league affiliate) and their AHL affiliate (the Texas\nStars).\nDiverse beam search: (1) That’s why I’m here.\n(2) That’s why I’m doing this.\nGFlowNet fine-tuning: (1) This is something I’ve always wanted.\n(2) When I was a teenager in middle school, I went to the Ice Arena\nin University of Houston and loved it.\nInput Prompt: That incident got a lot of attention in part because it was captured\non video. Israel said he recorded what happened at the synagogue,\nand made it public, to document it and leave no doubt about what\ntranspired.\nSampling w/ 𝑇= 1.0: (1) He blogged about it here as well.\n(2) Israeli TV stations broadcast the video before it aired in Israel,\nas the country’s rule requires.\nDiverse beam search: (1) However, there is no evidence that he did so.\n(2) However, there is no video of what happened inside the\nsynagogue.\nGFlowNet fine-tuning: (1) He is on tape doing all of it.\n(2) It’s a message countries can use to deter future attacks.\nInput Prompt: The Rolling Stones in Concert has since been released solely by\nABKCO Records. The band would remain incensed with Klein for\ndecades for that act. Klein died in 2009.\nSampling w/ 𝑇= 1.0: (1) Actually art has become as obscene as investment banking.\n(2) Some believe he shot himself in the chest.\nDiverse beam search: (1) The Rolling Stones, however, would not be deterred.\n(2) The Rolling Stones would go on to release their own version of\nThe Rolling Stones in Concert.\nGFlowNet fine-tuning: (1) He received a lifetime achievement award from the Jazz Times.\n(2) Sometimes it seems like we are destined to repeat our own\nmistakes.\n19\nPublished as a conference paper at ICLR 2024\nTable C.1: Examples of training samples for the stories infilling task.\nBeginning (𝑋) Middle (𝑍) End (𝑌)\nI was going to a Halloween party. I\nlooked through my clothes but could not\nfind a costume. I cut up my old clothes\nand constructed a costume.\nI put my costume on and\nwent to the party.\nMy friends loved my cos-\ntume.\nAllen thought he was a very talented\npoet. He attended college to study cre-\native writing. In college, he met a boy\nnamed Carl.\nCarl told him that he\nwasn’t very good.\nBecause of this, Allen\nswore off poetry forever.\nTable C.2: Hyperparameters for the story infilling task.\nLoRA rank 64\nLoRA scaling factor 16\nLoRA dropout 0.1\nBatch size 64\nGradient accumulation steps 16\nLearning rate 0.0001\nOptimizer AdamW\n𝑃𝐹 temperature max 2\n𝑃𝐹 temperature min 0.5\nReward temperature start 1.1\nReward temperature end 0.85\nReward temperature horizon 100\nBuffer capacity 25\nNumber of training steps 1000\nEvaluation temperature 0.8\nMaximum length 25\nMinimum length 5\n20\nPublished as a conference paper at ICLR 2024\nAnalysis. Table C.3, C.4, C.5, C.6, C.7, C.8 illustrate some examples of the infills generated with\nthe prompting baseline, supervised fine-tuned and GFlowNet fine-tuned models on the test examples.\nFor evaluating the infills we generate a rating for the stories with infills from each of the methods\nbased on the coherence of the story. We average the rating over 10 infills sampled from each method\nand average it over all the 100 test examples. The prompt used for evaluation is the following, adapted\nfrom Liu et al. (2023). Stories with infills generated by the GFlowNet fine-tuned model on average\nreceive a much higher rating than the baselines. The average rating for the reference infills is 4.3\nwhich should be viewed as an upper bound, as the stories may potentially be present in the GPT-4\ntraining data.\nYou will be given a short story.\nYour task is to rate the story on one metric.\nPlease make sure you read and understand these instructions\ncarefully. Please keep this document open while reviewing,\nand refer to it as needed.\nEvaluation Criteria:\nCoherence (1-5) - the collective quality of all sentences.\nThe story should be well-structured and well-organized.\nThe story should not just be a heap of related information,\nbut should build from sentence to a coherent narrative.\nThe story should also be realistic and all the sentences\nwhen put together should make sense.\nEvaluation Steps:\n1. Read the story carefully. Check if the story is coherent\nand all the sentences follow a logical order.\n2. Assign a score for coherence on a scale of 1 to 5,\nwhere 1 is the lowest and 5 is the highest based on the\nEvaluation Criteria. If there are grammatical errors\nor logical inconsistencies,\nsimply assign a lower score and do not elaborate on the reasoning.\nExample:\nRandy had recently separated from his wife.\nHe felt very lonely and sad all the time.\nHe considered trying to patch things up with Vera, his ex-wife.\nHe decided to get a puppy instead.\nHis decision made him happy and he no longer felt sad.\n- Coherence: 5\nRandy had recently separated from his wife.\nHe felt very lonely and sad all the time.\nHe considered trying to patch things up with Vera, his ex-wife.\nEventually Randy missed Vera very much and developedWhile making\nmemories with\nHis decision made him happy and he no longer felt sad.\n- Coherence 1\nStory:\n21\nPublished as a conference paper at ICLR 2024\n{{Story}}\nEvaluation Form (scores ONLY):\n- Coherence:\nD SUBJECTIVITY CLASSIFICATION\nAdditional details. See Table D.1 for some training examples in the SUBJ dataset. We use the\nprompts in Table D.2 for all baselines.\nWe run GFlowNet fine-tuning for 1000 steps with a linear warmup over 200 steps, a fixed learning\nrate of 0.0005, and a batch size of 512 samples; see Table D.3 for all the hyperparameters used. Each\nbatch consists of 8 queries (𝑋’s), randomly drawn with replacement. We then sample 64 rationales\n(𝑍’s) for every 𝑋, before using 𝑝LM (𝑍𝑌 |𝑋)as the reward. We also use the replay buffer described\nin §3.3 and seed it with potential rationales generated from 𝑝LM. For supervised fine-tuning, both on\nits own and on top of GFlowNet fine-tuning, we use a batch size of 256 with 8 queries, randomly\ndrawn with replacement. We train for 100 steps with a linear warm-up over 20 steps and a constant\nlearning rate. We sweep the learning rate in [0.001, 0.0001] and report the best performance. The\nreward temperature is annealed from 1.2 down to 1 over the first 150 steps of training.\nAdditional results. To encourage exploration in the space of 𝑍, we inverse-prompt the model to\ngenerate potential chains of thoughts and store them in a replay buffer at the beginning of training.\nThe replay buffer is a priority queue indexed by the reward. Under-performing chains of thoughts\nare evicted as we collect more trajectories. We ablate the effect of seeding the replay buffer with\ninverse-prompted chains of thoughts and aggregating multiple chains at test time in Table D.4.\nE INTEGER ARITHMETIC\nAdditional details. See Table E.1 for examples from synthetically generated training data for the\ninteger arithmetic task.\nAs with the infilling task, for the integer arithmetic task, the pretrained base models we tested were\nimperfect knowledge models, i.e. incorrect rationales are sometimes assigned very high likelihood.\nTo assign the correct rationales higher rewards, we prepend some demonstrations with hand-crafted\nrationales to the query when computing the reward, i.e. 𝑝LM (𝑋𝑍𝑌|(𝑋𝑖𝑍𝑖𝑌𝑖)𝑘\n𝑖=1)where 𝑘= 3 in our\nexperiments. This improves the calibration to some extent. Note that these (𝑋𝑖,𝑍𝑖,𝑌𝑖)are taken from\nthe dataset and used only in the reward. The GFlowNet policy here is conditioned only on 𝑋, i.e.,\n𝑞GFN (𝑍|𝑋).\nWe detail the hyperparameters used for training GFlowNets in our experiments in Table E.2. During\ntraining, we sample (𝑋,𝑌)from the dataset and then sample (batch size) 𝑍s for every (𝑋,𝑌), before\nusing 𝑝LM (𝑋𝑍𝑌|(𝑋𝑖𝑍𝑖𝑌𝑖)𝑘\n𝑖=1)as the reward. During the generation of 𝑍, as mentioned in §4.4,\nwhenever a “=” is generated we extract the preceding expression and evaluate the last two terms\nusing eval in Python. We use the replay buffer described in §3.3 and seed it with rationales from\nthe dataset 𝑝LM. We linearly anneal the reward temperature, and the learning rate during warmup.\nDuring evaluation, for all methods, we aggregate the response over multiple samples drawn from\nthe model at some fixed temperature. For the zero-shot baseline, we observe the best performance\nwith the “Let us think step by step.” prompt appended at the end of the question. For\nsupervised fine-tuning and PPO, we use the implementation from trl. For supervised fine-tuning\nwe use a batch size of 128 with 8 gradient accumulation steps and train for 10 epochs with a learning\nrate of 0.0001 using LoRA. For PPO we use a minibatch size of 64 with 16 gradient accumulation\nsteps, a learning rate of 0.0001, 4 epochs (on the minibatch), a clip range of 0.2, and an adaptive KL\ncoefficient initialized at 0.2.\nEffect of the number of rationales. As mentioned in §4.4, we seed the buffer with rationales from\na dataset. Table E.3 summarizes the results of ablation on the number of examples used to seed\nthe buffer. We observe, as expected, that the performance generally improves as we the number of\nexamples used to seed the buffer grows. When no buffers are used to seed the buffer, the performance\nis quite poor. We hypothesize that without the good rationales used to seed the buffer, the exploration\nproblem of searching for good rationales becomes very challenging.\n22\nPublished as a conference paper at ICLR 2024\nTable C.3: Examples of infills generated by the prompting baseline along with reference infills for the stories\ninfilling task.\nBeginning (𝑋) Middle (𝑍) End (𝑌)\nRandy had recently separated\nfrom his wife. He felt very\nlonely and sad all the time.\nHe considered trying to patch\nthings up with Vera, his\nex-wife.\nHe decided to get a puppy instead\nHe now happily had a new wife who he\nreally liked.\nHis wife missed him, but for the wrong\nreasons. Now\nHe finally found someone he could be\nhappy with. It was\nHe began spending more time with his\nnew girlfriend and friends.\nRandy remained single and mostly satis-\nfied with that. He found his\nWhen Randy was happy he went to the\naquarium, a place\nHe started going out more with a display\nof male affection.\nHe bought Vera a moonstone and gave it\nto her as\nHe and Vera got back together, even\nthough they had stayed\nHe introduced Paige to his new employ-\nees and they were great.\nRandy is happily married to Vera and\nthey have a healthy relationship\nHe found a new lady with a lot of the\nsame interests\nHis decision made\nhim happy and he\nno longer felt sad.\nKen put a bottle of beer in the\nfreezer. He heard a popping\nnoise. He looked in the\nfreezer and saw the bottle had\nburst.\nHe didn’t want to wait for another\nbeer to get cold\nHe went to sleep and came awake with\nan angry stomach.\nKen had another beer as a treat. After\nthe third beer\nHe stored the beer in the freezer and for-\ngot about it.\nHe walked around the bar a bit. He\nthinks he’ll\nWeeks later, Ken preferred his cold beer.\nHowever, he\nHe poured a fresh bottle with a sigh of\nrelief. It\nHe went to the bar for cold beer. No one\nknew\nKen hung up the phone on his friend. He\ndidn’t\nHe bought some veggies from the gro-\ncery store. It was warm\nKen watched the movie Goalkeeper. He\nwas glad the cold\nHe laid down for a rest. His belly was\nfull!\nHe laid down for the night and finally\nsleep again. The\nHe drank a warm\nbeer instead.\n23\nPublished as a conference paper at ICLR 2024\nTable C.4: Additional examples of infills generated by the prompting baseline along with reference infills for\nthe stories infilling task.\nBeginning (𝑋) Middle (𝑍) End (𝑌)\nJasmine had homework to do.\nShe did not have a pencil. She\nlooked around the room for\none.\nThe room did not have a pencil\nJasmine made her pencil and done her\nwork. She looked\nHer mom brought her a pencil. Jasmine\nwrote her homework\nJasmine took her pencil out of the room.\nShe now\nJasmine pretended not to have done her\nhomework. She got\nHer brother finally brought a pencil.\nThey could finally start!\nHer brother brought a pencil and solved\nher problem. Jasmine\nHer brother brought a pencil and solved\nher problem. Jasmine\nHer friend offered her a pencil. Jasmine\nused the pencil\nHer teacher saw her looking at her phone.\nThen, Jas\nHer mom came in and handed her the\nextra pencil! Jas\nHer mom brought her a pencil. Jasmine\nthen got to\nJasmine sent her friend money to do it.\nShe was\nJasmine decided not\nto do her\nhomework.\nJane had recently gotten a\nnew job. She was nervous\nabout her first day of work.\nOn the first day of work, Jane\noverslept.\nJane arrived at work an hour late\nDespite arriving late, Jane was greeted\nwarmly by her coworkers\nJane learned that you never can predict\nthe future. It was\nJane learned not to rush into anything.\nJane made a good\nMany of her peers were not pleased with\nher. Jane got\nJane ended up quitting her new job early.\nShe was happy\nMore time with the wrong people gave\nher anxiety. She was\nJane learned that you do not mess up at\na job well\nJane realized she needed to find new\nwork contacts.‘. At\nAfter her first day of work, Jane was too\ntired to\nSecretly, Jane hated her new job. It made\nher\nJane realized she worked at a mid-sized\ncompany with long\nJane’s boss realized she did not show up\nto work on\nJane did not make a\ngood impression at\nher new job.\n24\nPublished as a conference paper at ICLR 2024\nTable C.5: Examples of infills generated by the supervised fine-tuned model along with reference infills for the\nstories infilling task.\nBeginning (𝑋) Middle (𝑍) End (𝑌)\nRandy had recently separated\nfrom his wife. He felt very\nlonely and sad all the time.\nHe considered trying to patch\nthings up with Vera, his\nex-wife.\nHe decided to get a puppy instead\nHe started enjoying his time with Vera\neven more. Now,\nEventually Randy missed Vera very\nmuch and developedWhile making mem-\nories with\nHe finally decided he had enough pain\nfor no good reason.\nHe began spending more time with his\nnew girlfriend. Now he\nFor the first time since the divorce he\nstarted to make new\nSoon Randy and Vera were rekindling\ntheir romance. They\nHe started seeing someone new a day.\nNow, he feels\nHe signed up for a class in Same Sex\nLoving Uniqueness\nFor the first time in months, Randy and\nVera celebrated a\nHe introduced Paige to his new employ-\nees. They are now good\nAfter some contemplating, he decided to\npatch things up with Vera\nHe now looks for another partner to en-\njoy his life with.\nHis decision made\nhim happy and he\nno longer felt sad.\nKen put a bottle of beer in the\nfreezer. He heard a popping\nnoise. He looked in the\nfreezer and saw the bottle had\nburst.\nHe didn’t want to wait for another\nbeer to get cold\nThe popping noise was a freezer needle\ngetting caught. It fell\nKen had a cold for thirty minutes. After\nthe popping noise\nHe stored the beer in the freezer. The\npouring sound was\nHe didn’t know why it didn’t stop the\nringing.\nWeeks later, Ken knew the reason for the\nfreezing. It\nLater Ken learned the gas is out of gas\nand will end\nHe left the freezer for the beer. It stayed\nin there\nBefore he knew it, the popping noise was\ngone. He\nThe popping noise that Ken heard indi-\ncated it should have freezer shut\nThe popping noise became louder and\nmore frequent. Now Ken’s\nHe froze the beer and drank it from a\nmason jar\nHe went to the store to buy another. Now,\nhe\nHe drank a warm\nbeer instead.\n25\nPublished as a conference paper at ICLR 2024\nTable C.6: Additional examples of infills generated by the supervised fine-tuned model along with reference\ninfills for the stories infilling task.\nBeginning (𝑋) Middle (𝑍) End (𝑌)\nJasmine had homework to do.\nShe did not have a pencil. She\nlooked around the room for\none.\nThe room did not have a pencil\nJasmine decided to talk to her mom\nabout it. They\nHer dad now has to teach her math! Pick-\ning up\nJasmine took her math test later in the\nday. She\nJasmine got an A on her assignment. She\ncontinued to\nHer brother finally brought a pencil.\nThey could finally start!\nHer brother brought a pencil and solved\nher problem. Jasmine\nJasmine used her pink and brown high-\nlighter instead.\nJasmine went to school. She was able to\nfinish her\nJasmine made her own pencil. At school,\neveryone was\nHer brotherly earrings helped her stay\nup more. While\nJasmine made her own pencil. She ended\nup getting good\nJasmine sent her sister to do it. Now it\nwas\nJasmine decided not\nto do her\nhomework.\nJane had recently gotten a\nnew job. She was nervous\nabout her first day of work.\nOn the first day of work, Jane\noverslept.\nJane arrived at work an hour late\nAfter arriving home, Jane realized her\napartment was not ready.\nAfter work, Jane returned home and ate\na lousy sandwich.\nAfter her first day of work, Jane learned\nthat mistakes happen\nDue to her bad first day, Jane was fired\nfrom her\nEnding: Jane ended up late for work.\nAlthough she\nOn her 2nd day of work, Jane came\nhome late\nOn the second day of work, Jane arrived\nlate and unprepared\nBy the end of the day, Jane was tired and\nweary\nAfter her first day of work, Jane’s boss\nrecommended her\nSecretly, Jane hated her new job. Se-\ncretly,\nAfter work, Jane felt worn out and tired.\nThe end\nBy the end of the day Jane felt confident\nand like she\nJane did not make a\ngood impression at\nher new job.\n26\nPublished as a conference paper at ICLR 2024\nTable C.7: Examples of infills generated by the GFlowNet fine-tuned model along with reference infills for the\nstories infilling task.\nBeginning (𝑋) Middle (𝑍) End (𝑌)\nRandy had recently separated\nfrom his wife. He felt very\nlonely and sad all the time.\nHe considered trying to patch\nthings up with Vera, his\nex-wife.\nHe decided to get a puppy instead\nHe went to a friend’s house to talk.\nHe took Vera to the park for a picnic.\nHe finally decided to call Vera on the\nphone.\nHe asked Vera to meet him and they did.\nHe decided to get a dog for his family.\nHe went to Vera’s house.\nHe told Vera that he was ready to love\nagain.\nHe bought Vera a nice gift.\nHe had two dogs and felt like he missed\nVera.\nHe had a good couple of years.\nHe wanted to get Vera and he eventually\ndid.\nHe wanted to sit in a room alone.\nHis decision made\nhim happy and he\nno longer felt sad.\nKen put a bottle of beer in the\nfreezer. He heard a popping\nnoise. He looked in the\nfreezer and saw the bottle had\nburst.\nHe didn’t want to wait for another\nbeer to get cold\nHe decided to stop drinking beer.\nKen suddenly smelled a bottle of beer.\nHe thought it was something\nHe walked around the freezer to see the\nbottle.\nHe tried to clean up the freezer.\nHe was angry when he had to drink it.\nHe stepped down from the freezer\nHe made a big joke that night.\nHe looked in a jug and see it was, out.\nHe then looked for a beer the freezer.\nHe tried to enjoy the rest.\nHe watered the bottle,\nHe drank a warm\nbeer instead.\n27\nPublished as a conference paper at ICLR 2024\nTable C.8: Additional examples of infills generated by the GFlowNet fine-tuned model along with reference\ninfills for the stories infilling task.\nBeginning (𝑋) Middle (𝑍) End (𝑌)\nJasmine had homework to do.\nShe did not have a pencil. She\nlooked around the room for\none.\nThe room did not have a pencil\nShe was in a panic.\nShe had to place the pencil in her pocket.\nShe had several her classmates.\nShe looked for a neat pencil.\nShe thought she looked everywhere.\nHer brother walked in and borrowed her\npencil.\nShe only had a few minutes to do it.\nShe never had her pencil.\nShe saw a pen in her closet.\nShe searched everywhere for a pencil.\nShe went to her room.\nShe didn’t find one.\nJasmine decided not\nto do her\nhomework.\nJane had recently gotten a\nnew job. She was nervous\nabout her first day of work.\nOn the first day of work, Jane\noverslept.\nJane arrived at work an hour late\nShe lost her job after the first day of\nwork.\nShe was egged on by her boss.\nShe decided to over hyp her first day.\nShe arrived late and missed her first day\nof work.\nShe was nervous about going to work.\nShe ended up getting a good job.\nJane waited almost a day to get to work.\nHer first day was very difficult.\nShe made a couple of phone calls.\nShe arrived at the office and was fired.\nShe was all out of coffee.\nJane was surprised she did not make a\ngood impression.\nJane did not make a\ngood impression at\nher new job.\nTable D.1: Two training examples from the SUBJ dataset.\nText (𝑋) Label (𝑌)\nanother story follows the relationship between a stepfather ( neeson ) and his young stepson . hoffman ’s performance is authentic to the core of his being . objective\nsubjective\n28\nPublished as a conference paper at ICLR 2024\nTable D.2: Prompts used for subjectivity classification.\nFew-shot learning / Supervised fine-tuning GFlowNet fine-tuning\nClassify this movie review as objective or sub-\njective: “[𝑋]” This review is [𝑌].\nClassify this movie review as objective or sub-\njective: “[𝑋]” This review is [𝑍], so it is [𝑌].\nTable D.3: Hyperparameters for GFlowNet fine-tuning on subjectivity classification.\nLoRA rank 256\nLoRA scaling factor 16\nLoRA dropout 0.\nBatch size 16\nGradient accumulation steps 32\nLearning rate 0.0005\nOptimizer AdamW\n𝑃𝐹 temperature max 2\n𝑃𝐹 temperature min 0.5\nReward temperature start 1.2\nReward temperature end 1.0\nReward temperature horizon 150\nBuffer capacity 50\nNumber of steps 100\nNumber of samples 10\nMaximum length 5\nMinimum length 1\nAnalysis. In Table E.4 we show some examples generated by PPO and GFlowNet fine-tuned models.\nWe observe that PPO generates sequences with high rewards under the base model. These sequences,\nhowever, are not valid expressions, and, in fact, do not even call the tool. Instead, the model learns\nto simply repeat the expression. Repetitions being assigned high likelihood is an issue that has also\nbeen noted by Welleck et al. (2020). On the other hand, GFlowNets are able to generate the correct\nrationale to evaluate the expression.\nThere are still cases where the GFlowNet fails to produce the correct reasoning chain. We illustrate\nsome of these examples in Table E.5. The errors fall mainly into 3 categories: 1) missing operands in\nlonger OOD problems 2) incorrect operand being copied 3) incorrect operator being copied. The\npotential source for such errors is that the reward model assigns equally high rewards to rationales\nwith these minor mistakes and thus the model generates them with likelihood. These errors can\npotentially be reduced by using better and potentially larger reward models.\nTable D.4: Ablation studies on subjectivity classification using GPT-J 6.8B.\nMethod Test accuracy (%) ↑\nTraining samples\n10 20 50\nGFlowNet fine-tuning 71.4 ±1.8 81.1 ±0.4 87.7 ±2.2\n(-) Seed buffer 64.7 ±8.9 68.7 ±1.7 77.0 ±5.5\n(-) Seed buffer (-) Aggregating 63.9 ±7.1 65.9 ±2.4 75.4 ±3.3\n29\nPublished as a conference paper at ICLR 2024\nTable D.5: Top sample rationales for the SUBJ test set using the instruct-fine-tuned GPT-J 6B as both the reward\nmodel and the base model for the GFlowNet, which is trained on 50 labeled examples.\nTrue label Rationale Frequency\nThis review is about a factual event 11.45%\ndescribing a factual event 10.23%\nbased on a factual statement 9.47%\nObjective about a historical event 8.77%\nabout a movie review 7.41%\nbased on facts 5.64%\nabout a factual statement 4.61%\nThis review is about a movie review 26.21%\nabout the movie experience 26.17%\nabout the movie 7.60%\nSubjective describing a movie review 3.44%\nabout the movie review 2.31%\nbased on a fictional story 2.30%\ndescribing the movie experience 2.01%\nTable E.1: Examples from the integer arithmetic dataset. Note that the results of the two-term expressions are\nevaluated by the calculator.\nQuestion (𝑋) Rationale (𝑍) Answer (𝑌)\nQuestion: 6 - 0 - 4 - 8 = ? Answer: Question: 9 + 4 - 8 = ? Answer: 6 - 0 =, 6 - 4 =, 2 - 8 = 9 + 4 =, 13 - 8 = . The answer is -6.\n. The answer is 5.\nTable E.2: Hyperparameters for the Integer Arithmetic Task\nLoRA rank 64\nLoRA scaling factor 16\nLoRA dropout 0.1\nBatch size 16\nGradient accumulation steps 32\nLearning rate 0.0001\nOptimizer AdamW\n𝑃𝐹 temperature max 2\n𝑃𝐹 temperature min 0.5\nReward temperature start 1.1\nReward temperature end 0.5\nReward Temperature horizon 150\nBuffer capacity 50\nNumber of training steps 1000\nEvaluation temperature 0.1\nNumber of samples 10\nMaximum length 20\nMinimum length 5\nTable E.3: Ablation study to understand the effect of the number of rationales used to seed the buffer on the test\naccuracy (%) for the integer arithmetic task.\nNumber of operands\nIn-distribution OOD\nNumber of seed rationales 3 4 5\n0 22.6 5.8 3.4\n10 58.6 52.3 20.2\n25 48.8 56.8 24.6\n50 95.2 75.4 40.7\n30\nPublished as a conference paper at ICLR 2024\nTable E.4: Samples generated by PPO fine-tuned and GFlowNet fine-tuned models.\nQuestion (𝑋) Generated rationale (𝑍) log 𝑅\nQuestion: 1 - 9 + 8 = ?\nAnswer: 1 - 9 - 8 -13.17\n1 - 9 = -8, -8 + 8 = 0 -27.75\nQuestion: 8 + 7 + 2 + 7 = ?\nAnswer: 8 + 7 + 2 + 7 -2.39\n8 + 7 = 15, 15 + 2 = 17, 17 + 7 = 24 -11.72\nQuestion: 7 - 5 + 8 - 0 - 6 =?\nAnswer: 7 - 5 + -1.22\n7 - 5 = 2, 2 + 8 = 10, 10 - 0 = 10, 10 - 6\n= 4 -7.99\nTable E.5: Examples of errors made by the GFlowNet-fine-tuned model.\nQuestion (𝑋) Generated rationale (𝑍)\nQuestion: 7 + 3 + 4 - 9 + 9 =? Answer: 7 + 3 = 10, 10 - 9 = 1, 1 + 4 = 5\nQuestion: 8 + 5 - 1 - 4 + 5 =? Answer: 8 + 5 = 13, 13 - 1 = 12, 12 + 4 = 16, 16 -\n4 = 12\nQuestion: 0 + 5 - 6 - 1 - 6 =? Answer: 0 + 5 = 5, 5 - 6 = -1, -1 - 6 = -7, -7 + 1 =\n-6\n31\n}"]
